{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1361,
   "id": "f2e04f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from string import digits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score,confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_regression, f_classif\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "273a3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "id": "89d10f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('testdata.manual.2009.06.14.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "id": "2c4e33a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 1303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "id": "8b7249d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['polarity', 'tweet_id', 'date', 'query', 'user', 'tweet',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "id": "e7ed79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['polarity', 'tweet_id', 'date', 'query', 'user', 'tweet',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "id": "3a985347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Mon May 11 03:22:00 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>GeorgeVHulme</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  tweet_id                          date    query          user  \\\n",
       "0         4         4  Mon May 11 03:18:03 UTC 2009  kindle2        vcu451   \n",
       "1         4         5  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
       "2         4         6  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
       "3         4         7  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
       "4         4         8  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
       "\n",
       "                                               tweet  \n",
       "0  Reading my kindle2...  Love it... Lee childs i...  \n",
       "1  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "2  @kenburbary You'll love your Kindle2. I've had...  \n",
       "3  @mikefish  Fair enough. But i have the Kindle2...  \n",
       "4  @richardebaker no. it is too big. I'm quite ha...  "
      ]
     },
     "execution_count": 1306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "id": "9e610805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user                                              tweet  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03237584",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. Create sentiment column to classify the sentiment of tweets using the polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c3df9",
   "metadata": {},
   "source": [
    "According to the dataset, 0 means negative, 2 means neutral and 4 means positive. We do not need neutral tweets as they do not add value to our analysis of the tweet sentiment, so it would've been dropped, however, the dataset already had it dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "id": "1ea4ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 6) (799999, 6) (0, 6)\n"
     ]
    }
   ],
   "source": [
    "pos = df[df['polarity'] == 4] \n",
    "neg = df[df['polarity'] == 0]\n",
    "neutral = df[df['polarity']==2]\n",
    "print(pos.shape, neg.shape, neutral.shape)\n",
    "#Gives us insight into the ratings and how they shape the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "id": "ade76eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369784</th>\n",
       "      <td>4</td>\n",
       "      <td>2050886412</td>\n",
       "      <td>Fri Jun 05 19:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>insearchofnkotb</td>\n",
       "      <td>Joe and jon squeezing donnie's ass in the hudd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010430</th>\n",
       "      <td>4</td>\n",
       "      <td>1881000834</td>\n",
       "      <td>Fri May 22 03:15:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vastvision</td>\n",
       "      <td>Peter Martijn Wijnia - Sacrificies (Vast Visio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285474</th>\n",
       "      <td>0</td>\n",
       "      <td>1993620860</td>\n",
       "      <td>Mon Jun 01 10:40:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>keriwgd</td>\n",
       "      <td>So anxious about the Condo. I'm so hoping we d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423128</th>\n",
       "      <td>0</td>\n",
       "      <td>2062807404</td>\n",
       "      <td>Sun Jun 07 00:21:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kissypoo416</td>\n",
       "      <td>Pissed my bf stole my toothpaste GRRR.... It h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184770</th>\n",
       "      <td>4</td>\n",
       "      <td>1982665932</td>\n",
       "      <td>Sun May 31 11:39:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MouseGoesSqueak</td>\n",
       "      <td>@Anticlimatic Hehe. Speaking of eclipses, your...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity    tweet_id                          date     query  \\\n",
       "1369784         4  2050886412  Fri Jun 05 19:28:18 PDT 2009  NO_QUERY   \n",
       "1010430         4  1881000834  Fri May 22 03:15:14 PDT 2009  NO_QUERY   \n",
       "285474          0  1993620860  Mon Jun 01 10:40:14 PDT 2009  NO_QUERY   \n",
       "423128          0  2062807404  Sun Jun 07 00:21:56 PDT 2009  NO_QUERY   \n",
       "1184770         4  1982665932  Sun May 31 11:39:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1369784  insearchofnkotb  Joe and jon squeezing donnie's ass in the hudd...  \n",
       "1010430       vastvision  Peter Martijn Wijnia - Sacrificies (Vast Visio...  \n",
       "285474           keriwgd  So anxious about the Condo. I'm so hoping we d...  \n",
       "423128       kissypoo416  Pissed my bf stole my toothpaste GRRR.... It h...  \n",
       "1184770  MouseGoesSqueak  @Anticlimatic Hehe. Speaking of eclipses, your...  "
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df) #shuffled to make the data more represntative when used for the following operations of testing, training etc.\n",
    "df.head()\n",
    "#Reference: https://towardsdatascience.com/shuffling-rows-in-pandas-dataframes-eda052275635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "id": "e4b096b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_map = {0:0, 4:1} #Map out values, 1 and 2 should map to 0 meaning a negative review, and 4 and 5 map to 1 for pos.\n",
    "y = df['polarity'].map(y_map) #Now if we do df['column'].map we transform the data 1,2,4,5 into 0 and 1 making it binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "id": "aa6da90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "97d3fb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369784</th>\n",
       "      <td>4</td>\n",
       "      <td>2050886412</td>\n",
       "      <td>Fri Jun 05 19:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>insearchofnkotb</td>\n",
       "      <td>Joe and jon squeezing donnie's ass in the hudd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010430</th>\n",
       "      <td>4</td>\n",
       "      <td>1881000834</td>\n",
       "      <td>Fri May 22 03:15:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vastvision</td>\n",
       "      <td>Peter Martijn Wijnia - Sacrificies (Vast Visio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285474</th>\n",
       "      <td>0</td>\n",
       "      <td>1993620860</td>\n",
       "      <td>Mon Jun 01 10:40:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>keriwgd</td>\n",
       "      <td>So anxious about the Condo. I'm so hoping we d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423128</th>\n",
       "      <td>0</td>\n",
       "      <td>2062807404</td>\n",
       "      <td>Sun Jun 07 00:21:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kissypoo416</td>\n",
       "      <td>Pissed my bf stole my toothpaste GRRR.... It h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184770</th>\n",
       "      <td>4</td>\n",
       "      <td>1982665932</td>\n",
       "      <td>Sun May 31 11:39:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MouseGoesSqueak</td>\n",
       "      <td>@Anticlimatic Hehe. Speaking of eclipses, your...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity    tweet_id                          date     query  \\\n",
       "1369784         4  2050886412  Fri Jun 05 19:28:18 PDT 2009  NO_QUERY   \n",
       "1010430         4  1881000834  Fri May 22 03:15:14 PDT 2009  NO_QUERY   \n",
       "285474          0  1993620860  Mon Jun 01 10:40:14 PDT 2009  NO_QUERY   \n",
       "423128          0  2062807404  Sun Jun 07 00:21:56 PDT 2009  NO_QUERY   \n",
       "1184770         4  1982665932  Sun May 31 11:39:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \\\n",
       "1369784  insearchofnkotb  Joe and jon squeezing donnie's ass in the hudd...   \n",
       "1010430       vastvision  Peter Martijn Wijnia - Sacrificies (Vast Visio...   \n",
       "285474           keriwgd  So anxious about the Condo. I'm so hoping we d...   \n",
       "423128       kissypoo416  Pissed my bf stole my toothpaste GRRR.... It h...   \n",
       "1184770  MouseGoesSqueak  @Anticlimatic Hehe. Speaking of eclipses, your...   \n",
       "\n",
       "         sentiment  \n",
       "1369784          1  \n",
       "1010430          1  \n",
       "285474           0  \n",
       "423128           0  \n",
       "1184770          1  "
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "id": "2d6d3c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity      int64\n",
       "tweet_id      int64\n",
       "date         object\n",
       "query        object\n",
       "user         object\n",
       "tweet        object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 1313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5fc9",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a5d29",
   "metadata": {},
   "source": [
    "1. Convert to lowercase\n",
    "2. Rmove numbers\n",
    "3. Remove punctuation marks \n",
    "4. Remove HTML tags\n",
    "5. Convert emoticons to strings\n",
    "6. Tokenize the data\n",
    "7. Remove stop words\n",
    "8. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "1b91540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames_links(text):\n",
    "    text = re.sub('@[^\\s]+','',str(text))\n",
    "    text = re.sub('http[^\\s]+','',str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "id": "1a52cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "id": "43608914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nums(text):\n",
    "    no_nums = \"\".join([i for i in text if i not in string.digits])\n",
    "    return no_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "id": "af2efcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    no_puncts = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_puncts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "id": "eda6a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    no_html_tags = soup.get_text()\n",
    "    return no_html_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "id": "5d83a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    no_emoji = demojize(text)\n",
    "    return no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "id": "36cc1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "id": "f9a050af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "e3018db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "6f042555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in en_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "137f7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "id": "1a326945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(text):\n",
    "    lemmatized_text = \" \".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "id": "1eda054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = text.apply(remove_usernames_links)\n",
    "    cleaned_text = cleaned_text.apply(convert_to_lowercase)\n",
    "    cleaned_text = cleaned_text.apply(remove_nums)\n",
    "    cleaned_text = cleaned_text.apply(remove_emojis) \n",
    "    cleaned_text = cleaned_text.apply(remove_punctuations)\n",
    "    cleaned_text = cleaned_text.apply(remove_html_tags)\n",
    "    cleaned_text = cleaned_text.apply(tokenizer.tokenize)\n",
    "    cleaned_text = cleaned_text.apply(lambda x: remove_stopwords(x))\n",
    "    cleaned_text = cleaned_text.apply(lambda x: word_lemmatizer(x))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "id": "0c0815ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    cleaned_text = []\n",
    "    \n",
    "    #text = text.lower()\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_nums(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = word_lemmatizer(text)\n",
    "    \n",
    "    cleaned_text.append(text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "id": "535946ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369784    Joe and jon squeezing donnie's ass in the hudd...\n",
       "1010430    Peter Martijn Wijnia - Sacrificies (Vast Visio...\n",
       "285474     So anxious about the Condo. I'm so hoping we d...\n",
       "423128     Pissed my bf stole my toothpaste GRRR.... It h...\n",
       "1184770    @Anticlimatic Hehe. Speaking of eclipses, your...\n",
       "                                 ...                        \n",
       "20382                              Still can't fall asleep. \n",
       "272275     @Tarale That's really bad, very sorry 4 you.  ...\n",
       "1479373    happy that Boom Boom Pow is number 1 in the UK...\n",
       "143503                                Is tired from traffic \n",
       "501401     I miss my younger days when &quot;the last day...\n",
       "Name: tweet, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 1329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftext = df['tweet']\n",
    "dftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "id": "0cc197c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = clean_text(dftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "id": "eb60ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "id": "06c5df5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369784</th>\n",
       "      <td>4</td>\n",
       "      <td>2050886412</td>\n",
       "      <td>Fri Jun 05 19:28:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>insearchofnkotb</td>\n",
       "      <td>Joe and jon squeezing donnie's ass in the hudd...</td>\n",
       "      <td>1</td>\n",
       "      <td>joe jon squeezing donnies as huddle bout nd pose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010430</th>\n",
       "      <td>4</td>\n",
       "      <td>1881000834</td>\n",
       "      <td>Fri May 22 03:15:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vastvision</td>\n",
       "      <td>Peter Martijn Wijnia - Sacrificies (Vast Visio...</td>\n",
       "      <td>1</td>\n",
       "      <td>peter martijn wijnia sacrificies vast vision r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285474</th>\n",
       "      <td>0</td>\n",
       "      <td>1993620860</td>\n",
       "      <td>Mon Jun 01 10:40:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>keriwgd</td>\n",
       "      <td>So anxious about the Condo. I'm so hoping we d...</td>\n",
       "      <td>0</td>\n",
       "      <td>anxious condo im hoping dont lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423128</th>\n",
       "      <td>0</td>\n",
       "      <td>2062807404</td>\n",
       "      <td>Sun Jun 07 00:21:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kissypoo416</td>\n",
       "      <td>Pissed my bf stole my toothpaste GRRR.... It h...</td>\n",
       "      <td>0</td>\n",
       "      <td>pissed bf stole toothpaste grrr oxidating bubb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184770</th>\n",
       "      <td>4</td>\n",
       "      <td>1982665932</td>\n",
       "      <td>Sun May 31 11:39:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MouseGoesSqueak</td>\n",
       "      <td>@Anticlimatic Hehe. Speaking of eclipses, your...</td>\n",
       "      <td>1</td>\n",
       "      <td>hehe speaking eclipse vagina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity    tweet_id                          date     query  \\\n",
       "1369784         4  2050886412  Fri Jun 05 19:28:18 PDT 2009  NO_QUERY   \n",
       "1010430         4  1881000834  Fri May 22 03:15:14 PDT 2009  NO_QUERY   \n",
       "285474          0  1993620860  Mon Jun 01 10:40:14 PDT 2009  NO_QUERY   \n",
       "423128          0  2062807404  Sun Jun 07 00:21:56 PDT 2009  NO_QUERY   \n",
       "1184770         4  1982665932  Sun May 31 11:39:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \\\n",
       "1369784  insearchofnkotb  Joe and jon squeezing donnie's ass in the hudd...   \n",
       "1010430       vastvision  Peter Martijn Wijnia - Sacrificies (Vast Visio...   \n",
       "285474           keriwgd  So anxious about the Condo. I'm so hoping we d...   \n",
       "423128       kissypoo416  Pissed my bf stole my toothpaste GRRR.... It h...   \n",
       "1184770  MouseGoesSqueak  @Anticlimatic Hehe. Speaking of eclipses, your...   \n",
       "\n",
       "         sentiment                                      cleaned_tweet  \n",
       "1369784          1   joe jon squeezing donnies as huddle bout nd pose  \n",
       "1010430          1  peter martijn wijnia sacrificies vast vision r...  \n",
       "285474           0                  anxious condo im hoping dont lose  \n",
       "423128           0  pissed bf stole toothpaste grrr oxidating bubb...  \n",
       "1184770          1                       hehe speaking eclipse vagina  "
      ]
     },
     "execution_count": 1333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "id": "4bad3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TEST CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857a18d",
   "metadata": {},
   "source": [
    "***\n",
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "id": "1d9e8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: Lab 5\n",
    "def text_fit(X, y, model, clf_model, coef_show=1): \n",
    "    X_c = model.fit_transform(X) \n",
    "    print('# features: {}'.format(X_c.shape[1]))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0) #Splits the data into one for training and one for testing\n",
    "    print('# train records: {}'.format(X_train.shape[0]))\n",
    "    print('# test records: {}'.format(X_test.shape[0]))\n",
    "    clf = clf_model.fit(X_train, y_train) #Fit the logistic reg model with the training data\n",
    "    y_pred = clf.predict(X_test)  #Prediction using the Test data\n",
    "    if coef_show == 1:  #Extract the coefficients from the model and put it in a dataframe\n",
    "        w = model.get_feature_names()\n",
    "        coef = clf.coef_.tolist()[0]\n",
    "        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})\n",
    "        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "        print('')\n",
    "        print('-Top 20 positive-')\n",
    "        print(coeff_df.head(20).to_string(index=False))\n",
    "        print('')\n",
    "        print('-Top 20 negative-')        \n",
    "        print(coeff_df.tail(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "id": "b589f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_tweet']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "id": "ec12cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_n = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "id": "cb62b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features: 4397916\n",
      "# train records: 1199999\n",
      "# test records: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Top 20 positive-\n",
      "             Word  Coefficient\n",
      "        cant wait     5.445716\n",
      "      cannot wait     4.234849\n",
      "      doesnt hurt     3.437968\n",
      "wont disappointed     3.297625\n",
      "        wish luck     3.218377\n",
      "         aint bad     3.165187\n",
      "    nothing wrong     3.131649\n",
      "        wont hurt     3.052176\n",
      "       get enough     3.047692\n",
      "      sad anymore     2.910309\n",
      "          smiling     2.865787\n",
      "            smile     2.791474\n",
      "         cant bad     2.787228\n",
      "    headache gone     2.716984\n",
      "        canâ wait     2.706236\n",
      "         isnt bad     2.680736\n",
      "        wasnt bad     2.633715\n",
      "      wont regret     2.625691\n",
      "      take credit     2.609796\n",
      "          sad sad     2.595777\n",
      "\n",
      "-Top 20 negative-\n",
      "         Word  Coefficient\n",
      "       broken    -3.494878\n",
      "       missin    -3.563985\n",
      "disappointing    -3.650849\n",
      "  passed away    -3.658302\n",
      "     headache    -3.658768\n",
      "unfortunately    -3.727078\n",
      "        upset    -3.811648\n",
      "         cant    -3.872348\n",
      "         lost    -3.886955\n",
      "       ruined    -3.931638\n",
      "       bummer    -3.949663\n",
      "    cancelled    -4.005412\n",
      "       bummed    -4.165897\n",
      "          rip    -4.190455\n",
      "         died    -4.221696\n",
      "         sick    -4.237195\n",
      "        sadly    -4.420610\n",
      "      missing    -4.557099\n",
      "         poor    -4.950303\n",
      "          sad    -6.383237\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_n = text_fit(X, y, tfidf_n, LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "id": "4a679f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features: 4397916\n",
      "# train records: 1199999\n",
      "# test records: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Top 20 positive-\n",
      "          Word  Coefficient\n",
      "     cant wait    13.678785\n",
      "   cannot wait    11.512841\n",
      "     wish luck     9.485170\n",
      "       smiling     8.558742\n",
      "     wasnt bad     8.157187\n",
      "congratulation     8.011944\n",
      " nothing wrong     7.822763\n",
      "      isnt bad     7.734759\n",
      "          glad     7.164461\n",
      "     dont miss     6.990786\n",
      "    get enough     6.960296\n",
      "         smile     6.955471\n",
      "       welcome     6.930817\n",
      "          love     6.918243\n",
      "       blessed     6.877666\n",
      "       excited     6.844365\n",
      "         thank     6.795414\n",
      "       amazing     6.784287\n",
      "   dont forget     6.762348\n",
      "         proud     6.564884\n",
      "\n",
      "-Top 20 negative-\n",
      "         Word  Coefficient\n",
      "unfortunately   -10.431982\n",
      "         hurt   -10.443813\n",
      "        upset   -10.626204\n",
      "       bummer   -10.693473\n",
      "         lost   -10.701497\n",
      "       ruined   -10.715436\n",
      "    depressed   -10.721984\n",
      "   depressing   -10.746080\n",
      "         miss   -10.805262\n",
      " disappointed   -10.845649\n",
      "         poor   -11.110749\n",
      "    cancelled   -11.329789\n",
      "       gutted   -11.489428\n",
      "         cant   -11.542699\n",
      "      missing   -11.785974\n",
      "         sick   -12.033114\n",
      "          rip   -12.297027\n",
      "       bummed   -12.506581\n",
      "        sadly   -13.516838\n",
      "          sad   -20.657350\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf_n = text_fit(X, y, tfidf_n, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ff7fb",
   "metadata": {},
   "source": [
    "***\n",
    "### Topic Modelling Using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "id": "d3745edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: Lab 5\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print(\"\\n\")\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "id": "cd7f91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1599999\n"
     ]
    }
   ],
   "source": [
    "documents = list(X)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "255c0a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF model using NMF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "07258299",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "id": "165f6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "id": "647a2c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Topics \n",
      "\n",
      "\n",
      "Topic 0:\n",
      "day today happy great mother last school another beautiful long nice birthday tomorrow bad everyone hope first sunny mom father\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "day\n",
      "\n",
      "\n",
      "selfdeclared day\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "good morning night luck everyone thats hope sound feel feeling thing time today world pretty afternoon monday weekend idea say\n",
      "\n",
      "\n",
      "good night good morning\n",
      "\n",
      "\n",
      "good morning good night\n",
      "\n",
      "\n",
      "good night good morning\n",
      "\n",
      "\n",
      "good morning good night\n",
      "\n",
      "\n",
      "good morning good night\n",
      "\n",
      "\n",
      "good night good morning\n",
      "\n",
      "\n",
      "good morning good night\n",
      "\n",
      "\n",
      "good morning good night\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "thanks follow following followfriday much great ff hey haha guy ill lol awesome aww lot ok link appreciate cool sharing\n",
      "\n",
      "\n",
      "royhooker thanks\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "thanks quotfucktasticquot\n",
      "\n",
      "\n",
      "thanks\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "im sorry going tired gonna sad bored sick sure still getting think happy right excited glad bed hungry hear home\n",
      "\n",
      "\n",
      "im bitcham\n",
      "\n",
      "\n",
      "im tnulb\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "im\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "get cant like time lol see got one well going really today sleep oh wait night hope need new feel\n",
      "\n",
      "\n",
      "cant get time\n",
      "\n",
      "\n",
      "cant sleep get lol\n",
      "\n",
      "\n",
      "cant wait get see\n",
      "\n",
      "\n",
      "cant get\n",
      "\n",
      "\n",
      "cant get\n",
      "\n",
      "\n",
      "cant get\n",
      "\n",
      "\n",
      "cant get\n",
      "\n",
      "\n",
      "cant get beamagpie\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "work back today tomorrow ready hour going getting doesnt still weekend got tired early home morning way gotta didnt week\n",
      "\n",
      "\n",
      "work work work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "work\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "love much song thank would lt guy haha new show movie happy amazing life watching girl ya id friend music\n",
      "\n",
      "\n",
      "love logoes\n",
      "\n",
      "\n",
      "love\n",
      "\n",
      "\n",
      "love\n",
      "\n",
      "\n",
      "love\n",
      "\n",
      "\n",
      "love\n",
      "\n",
      "\n",
      "love gooseness\n",
      "\n",
      "\n",
      "love u\n",
      "\n",
      "\n",
      "love x\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "know dont think feel like want let wanna even really right anymore worry people mean didnt say make haha lol\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know writeeee\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "dont know\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "go want home back wanna bed school sleep ready tomorrow really dont away come could wish tired see gonna rain\n",
      "\n",
      "\n",
      "want go go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go\n",
      "\n",
      "\n",
      "want go bayarea\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "miss much already friend gonna going baby guy home really back ill come girl lt school boyfriend boy lot old\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n",
      "miss faiz\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n",
      "miss cullies\n",
      "\n",
      "\n",
      "miss\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 20\n",
    "no_top_documents = 8\n",
    "print(\"NMF Topics \\n\\n\")\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d40a5",
   "metadata": {},
   "source": [
    "***\n",
    "### Classifying whether a user's tweet has a negative or positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "id": "9fadd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "id": "bbaa4da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 1347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_n.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "id": "35b10060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_n.transform(X_train)\n",
    "X_test  = tfidf_n.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "id": "76550448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(clf_model):\n",
    "    \n",
    "    y_pred = clf_model.predict(X_test)\n",
    "    recall = recall_score(y_test,y_pred) #Calculate the recall score between actual predictions and model predictions\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print ('Model Recall: {}'.format(recall))\n",
    "    print ('Model Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "id": "f478f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79     39880\n",
      "           1       0.79      0.79      0.79     40120\n",
      "\n",
      "    accuracy                           0.79     80000\n",
      "   macro avg       0.79      0.79      0.79     80000\n",
      "weighted avg       0.79      0.79      0.79     80000\n",
      "\n",
      "Model Recall: 0.7944915254237288\n",
      "Model Accuracy: 0.7925\n"
     ]
    }
   ],
   "source": [
    "svc_model = LinearSVC().fit(X_train, y_train)\n",
    "assess(svc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "id": "36c3efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80     39880\n",
      "           1       0.80      0.80      0.80     40120\n",
      "\n",
      "    accuracy                           0.80     80000\n",
      "   macro avg       0.80      0.80      0.80     80000\n",
      "weighted avg       0.80      0.80      0.80     80000\n",
      "\n",
      "Model Recall: 0.803988035892323\n",
      "Model Accuracy: 0.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lgr_model = LogisticRegression().fit(X_train, y_train)\n",
    "assess(lgr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "id": "7fced2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(tfidf_n, model, text):\n",
    "    tweet = tfidf_n.transform(clean(text))\n",
    "    sentiment = model.predict(tweet)\n",
    "    posp = \"Positive Sentiment\"\n",
    "    negp = \"Negative Sentiment\"\n",
    "    \n",
    "    if sentiment == 0:\n",
    "        return negp\n",
    "    if sentiment == 1:\n",
    "        return posp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "id": "20596629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Sentiment'"
      ]
     },
     "execution_count": 1357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter your tweet to predict the sentiment:\n",
    "tweet = [\"I hate puppies\"]\n",
    "predict_sentiment(tfidf_n, svc_model, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "id": "da237644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Sentiment'"
      ]
     },
     "execution_count": 1358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter your tweet to predict the sentiment:\n",
    "tweet = [\"I love puppies\"]\n",
    "predict_sentiment(tfidf_n, svc_model, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "id": "3aeed5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Sentiment'"
      ]
     },
     "execution_count": 1355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter your tweet to predict the sentiment:\n",
    "tweet = [\"I hate puppies\"]\n",
    "predict_sentiment(tfidf_n, lgr_model, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "id": "683ebfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Sentiment'"
      ]
     },
     "execution_count": 1359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter your tweet to predict the sentiment:\n",
    "tweet = [\"I love puppies\"]\n",
    "predict_sentiment(tfidf_n, lgr_model, tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e454be",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* Lab 5\n",
    "* https://dylancastillo.co/nlp-snippets-clean-and-tokenize-text-with-python/#transform-emojis-to-characters\n",
    "* https://machinelearningmastery.com/how-to-connect-model-input-data-with-predictions-for-machine-learning/\n",
    "* https://www.academia.edu/74585014/Machine_Learning_Approach_to_Sentiment_Analysis_from_Movie_Reviews_Using_Word2Vec\n",
    "* https://entertainment.bacsigan.com/popedaze/sentiment-analysis-in-python\n",
    "* https://www.w3schools.com/python/python_lists_add.asp\n",
    "* https://stackoverflow.com/questions/32106063/sklearn-linearsvc-x-has-1-features-per-sample-expecting-5\n",
    "* https://datascience.stackexchange.com/questions/51224/why-does-transform-from-tfidf-vectorizer-sklearn-not-work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854391b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
